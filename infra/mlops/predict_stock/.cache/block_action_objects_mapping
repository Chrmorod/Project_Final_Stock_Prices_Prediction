{"block_file": {"predict_stock/data_exporters/export_data.py:data_exporter:python:predict stock/data exporters/export data": {"content": "import mlflow\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data(data, data_2, data_3, *args, **kwargs):\n    expected_price_MonteCarlo = data.get('last_expected_price_montecarlo')\n    expected_price_ARIMA = data_2.get('last_expected_price_arima')\n    expected_price_LSTM = data_3.get('last_expected_price_lstm')\n    average_expected_price_MonteCarlo = data.get('average_expected_price_montecarlo')\n    average_expected_price_ARIMA = data_2.get('average_expected_price_arima')\n    average_expected_price_LSTM = data_3.get('average_expected_price_lstm')\n\n    hybrid_expected_price = (\n        float(expected_price_MonteCarlo) + \n        float(expected_price_ARIMA) + \n        float(expected_price_LSTM)\n    ) / 3\n\n    hybrid_average_expected_price = (\n        float(average_expected_price_MonteCarlo) + \n        float(average_expected_price_ARIMA) + \n        float(average_expected_price_LSTM)\n    ) / 3\n    mlflow.set_tracking_uri(\"http://mlflow:5000\")\n    mlflow.set_experiment(\"hybrid_prediction_price\")\n\n    with mlflow.start_run():\n        mlflow.log_param(\"model_1\", \"MonteCarlo\")\n        mlflow.log_param(\"model_2\", \"ARIMA\")\n        mlflow.log_param(\"model_3\", \"LSTM\")\n\n        mlflow.log_metric(\"expected_price_montecarlo\", expected_price_MonteCarlo)\n        mlflow.log_metric(\"expected_price_arima\", expected_price_ARIMA)\n        mlflow.log_metric(\"expected_price_lstm\", expected_price_LSTM)\n        mlflow.log_metric(\"hybrid_expected_price\", hybrid_expected_price)\n\n        mlflow.log_metric(\"average_expected_price_montecarlo\", average_expected_price_MonteCarlo)\n        mlflow.log_metric(\"average_expected_price_arima\", average_expected_price_ARIMA)\n        mlflow.log_metric(\"average_expected_price_lstm\", average_expected_price_LSTM)\n        mlflow.log_metric(\"hybrid_average_expected_price\", hybrid_average_expected_price)\n\n    return {\n        'hybrid_expected_price': hybrid_expected_price,\n        'hybrid_average_expected_price': hybrid_average_expected_price\n    }\n", "file_path": "predict_stock/data_exporters/export_data.py", "language": "python", "type": "data_exporter", "uuid": "predict_stock/data_exporters/export_data"}, "predict_stock/data_exporters/export_data_t.py:data_exporter:python:predict stock/data exporters/export data t": {"content": "import mlflow\n\ntry:\n    from mage_ai.data_preparation.decorators import data_exporter\nexcept ImportError:\n    def data_exporter(func):\n        return func\n\n\n@data_exporter\ndef export_data(data, data_2, data_3, *args, **kwargs):\n    expected_price_MonteCarlo = data.get('last_expected_price_montecarlo')\n    expected_price_ARIMA = data_2.get('last_expected_price_arima')\n    expected_price_LSTM = data_3.get('last_expected_price_lstm')\n    average_expected_price_MonteCarlo = data.get('average_expected_price_montecarlo')\n    average_expected_price_ARIMA = data_2.get('average_expected_price_arima')\n    average_expected_price_LSTM = data_3.get('average_expected_price_lstm')\n\n    hybrid_expected_price = (\n        float(expected_price_MonteCarlo) + \n        float(expected_price_ARIMA) + \n        float(expected_price_LSTM)\n    ) / 3\n\n    hybrid_average_expected_price = (\n        float(average_expected_price_MonteCarlo) + \n        float(average_expected_price_ARIMA) + \n        float(average_expected_price_LSTM)\n    ) / 3\n    mlflow.set_tracking_uri(\"http://mlflow:5000\")\n    mlflow.set_experiment(\"hybrid_prediction_price\")\n\n    with mlflow.start_run():\n        mlflow.log_param(\"model_1\", \"MonteCarlo\")\n        mlflow.log_param(\"model_2\", \"ARIMA\")\n        mlflow.log_param(\"model_3\", \"LSTM\")\n\n        mlflow.log_metric(\"expected_price_montecarlo\", expected_price_MonteCarlo)\n        mlflow.log_metric(\"expected_price_arima\", expected_price_ARIMA)\n        mlflow.log_metric(\"expected_price_lstm\", expected_price_LSTM)\n        mlflow.log_metric(\"hybrid_expected_price\", hybrid_expected_price)\n\n        mlflow.log_metric(\"average_expected_price_montecarlo\", average_expected_price_MonteCarlo)\n        mlflow.log_metric(\"average_expected_price_arima\", average_expected_price_ARIMA)\n        mlflow.log_metric(\"average_expected_price_lstm\", average_expected_price_LSTM)\n        mlflow.log_metric(\"hybrid_average_expected_price\", hybrid_average_expected_price)\n\n    return {\n        'hybrid_expected_price': hybrid_expected_price,\n        'hybrid_average_expected_price': hybrid_average_expected_price\n    }\n", "file_path": "predict_stock/data_exporters/export_data_t.py", "language": "python", "type": "data_exporter", "uuid": "predict_stock/data_exporters/export_data_t"}, "predict_stock/data_exporters/__init__.py:data_exporter:python:predict stock/data exporters/  init  ": {"content": "", "file_path": "predict_stock/data_exporters/__init__.py", "language": "python", "type": "data_exporter", "uuid": "predict_stock/data_exporters/__init__"}, "predict_stock/data_loaders/data_extract.py:data_loader:python:predict stock/data loaders/data extract": {"content": "if 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n@data_loader\ndef extract_data(**kwargs):\n    from datetime import datetime\n    import yfinance as yf\n    import pandas as pd\n    import mlflow\n    import os\n\n    stock = kwargs.get('stock') or os.getenv('STOCK', 'NT5.F')\n    year_back = int(kwargs.get('year_back') or os.getenv('YEAR_BACK', 4))\n    \n    #stock = kwargs['stock']\n    #year_back = int(kwargs['year_back'])\n    \n    mlflow.set_tracking_uri(\"http://mlflow:5000\")\n    mlflow.set_experiment(f\"extract_data__{stock}_finance_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n\n    end = datetime.now()\n    start = datetime(end.year - year_back, end.month, end.day)\n\n    df = yf.download(stock, start=start, end=end)\n\n    df_close = df[['Close']].copy().reset_index()\n\n    with mlflow.start_run(run_name=f\"extract_{stock}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n        mlflow.log_param(\"stock\", stock)\n        mlflow.log_param(\"year_back\", year_back)\n        mlflow.log_param(\"start_date\", start.strftime('%Y-%m-%d'))\n        mlflow.log_param(\"end_date\", end.strftime('%Y-%m-%d'))\n        mlflow.log_metric(\"num_rows\", df_close.shape[0])\n        mlflow.log_metric(\"num_columns\", df_close.shape[1])\n\n    if isinstance(df_close.columns, pd.MultiIndex):\n        df_close.columns = df_close.columns.get_level_values(0)  #first level\n\n    df_close.columns = [str(c) for c in df_close.columns]\n\n    return df_close", "file_path": "predict_stock/data_loaders/data_extract.py", "language": "python", "type": "data_loader", "uuid": "predict_stock/data_loaders/data_extract"}, "predict_stock/data_loaders/data_extract_t.py:data_loader:python:predict stock/data loaders/data extract t": {"content": "try:\n    from mage_ai.data_preparation.decorators import data_loader\nexcept ImportError:\n    def data_loader(func):\n        return func\nfrom datetime import datetime\nimport yfinance as yf\nimport pandas as pd\nimport mlflow\nimport os\n@data_loader\ndef extract_data(**kwargs):\n    stock = kwargs.get('stock') or os.getenv('STOCK', 'NT5.F')\n    year_back = int(kwargs.get('year_back') or os.getenv('YEAR_BACK', 4))\n    \n    #stock = kwargs['stock']\n    #year_back = int(kwargs['year_back'])\n    \n    mlflow.set_tracking_uri(\"http://mlflow:5000\")\n    mlflow.set_experiment(f\"extract_data__{stock}_finance_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n\n    end = datetime.now()\n    start = datetime(end.year - year_back, end.month, end.day)\n\n    df = yf.download(stock, start=start, end=end)\n\n    df_close = df[['Close']].copy().reset_index()\n\n    with mlflow.start_run(run_name=f\"extract_{stock}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n        mlflow.log_param(\"stock\", stock)\n        mlflow.log_param(\"year_back\", year_back)\n        mlflow.log_param(\"start_date\", start.strftime('%Y-%m-%d'))\n        mlflow.log_param(\"end_date\", end.strftime('%Y-%m-%d'))\n        mlflow.log_metric(\"num_rows\", df_close.shape[0])\n        mlflow.log_metric(\"num_columns\", df_close.shape[1])\n\n    if isinstance(df_close.columns, pd.MultiIndex):\n        df_close.columns = df_close.columns.get_level_values(0)  #first level\n\n    df_close.columns = [str(c) for c in df_close.columns]\n\n    return df_close", "file_path": "predict_stock/data_loaders/data_extract_t.py", "language": "python", "type": "data_loader", "uuid": "predict_stock/data_loaders/data_extract_t"}, "predict_stock/data_loaders/__init__.py:data_loader:python:predict stock/data loaders/  init  ": {"content": "", "file_path": "predict_stock/data_loaders/__init__.py", "language": "python", "type": "data_loader", "uuid": "predict_stock/data_loaders/__init__"}, "predict_stock/transformers/arima.py:transformer:python:predict stock/transformers/arima": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport mlflow\nimport tempfile\nimport os\nimport cloudpickle\n\n@transformer\ndef transform(data):\n    import numpy as np\n    import pandas as pd\n    from datetime import datetime\n    from statsmodels.tsa.arima.model import ARIMA\n    import warnings\n\n    warnings.simplefilter(action='ignore', category=FutureWarning)\n\n    data.index = pd.to_datetime(data.index)\n    \n    prices = data['Close']\n    n_days = 30 \n\n    model = ARIMA(prices, order=(5, 1, 0))\n    model_fit = model.fit()\n\n    future_preds = model_fit.forecast(steps=n_days)\n    last_date = prices.index[-1]\n    future_dates = pd.bdate_range(start=last_date + pd.Timedelta(days=1), periods=n_days)\n\n    future_forecast = pd.DataFrame({'Date': future_dates, 'Forecast': future_preds})\n    future_forecast.set_index('Date', inplace=True)\n\n    expected_price = future_preds.iloc[-1]\n    average_expected_price = future_preds.mean()\n    try:\n        mlflow.set_tracking_uri(\"http://mlflow:5000\")\n        mlflow.set_experiment(f\"ARIMA_{n_days}d_{last_date}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n\n        with mlflow.start_run(run_name=f\"Run_{last_date.strftime('%Y-%m-%d')}\"):\n            mlflow.log_param(\"model\", \"ARIMA(5,1,0)\")\n            mlflow.log_param(\"n_days\", n_days)\n            mlflow.log_param(\"last_date\", str(last_date))\n\n            mlflow.log_metric(\"expected_price\", expected_price)\n            mlflow.log_metric(\"average_expected_price\", average_expected_price)\n\n            mlflow.statsmodels.log_model(\n                statsmodels_model=model_fit,\n                artifact_path=\"arima_model\",\n                registered_model_name=\"ARIMA_Stock_Predictor\"\n            )\n\n        if mlflow.active_run():\n            mlflow.end_run()\n    except Exception as e:\n        print(f\"MLflow error: {e}\")\n    return {\n        'last_expected_price_arima': expected_price,\n        'average_expected_price_arima': average_expected_price,\n    }", "file_path": "predict_stock/transformers/arima.py", "language": "python", "type": "transformer", "uuid": "predict_stock/transformers/arima"}, "predict_stock/transformers/arima_t.py:transformer:python:predict stock/transformers/arima t": {"content": "try:\n    from mage_ai.data_preparation.decorators import transformer\nexcept ImportError:\n    def transformer(func):\n        return func\n\nimport mlflow\nimport tempfile\nimport cloudpickle\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom statsmodels.tsa.arima.model import ARIMA\nimport warnings\n\n@transformer\ndef transform(data):\n    warnings.simplefilter(action='ignore', category=FutureWarning)\n\n    data.index = pd.to_datetime(data.index)\n    \n    prices = data['Close']\n    n_days = 30 \n\n    model = ARIMA(prices, order=(5, 1, 0))\n    model_fit = model.fit()\n\n    future_preds = model_fit.forecast(steps=n_days)\n    last_date = prices.index[-1]\n    future_dates = pd.bdate_range(start=last_date + pd.Timedelta(days=1), periods=n_days)\n\n    future_forecast = pd.DataFrame({'Date': future_dates, 'Forecast': future_preds})\n    future_forecast.set_index('Date', inplace=True)\n\n    expected_price = future_preds.iloc[-1]\n    average_expected_price = future_preds.mean()\n    try:\n        mlflow.set_tracking_uri(\"http://mlflow:5000\")\n        mlflow.set_experiment(f\"ARIMA_{n_days}d_{last_date}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n\n        with mlflow.start_run(run_name=f\"Run_{last_date.strftime('%Y-%m-%d')}\"):\n            mlflow.log_param(\"model\", \"ARIMA(5,1,0)\")\n            mlflow.log_param(\"n_days\", n_days)\n            mlflow.log_param(\"last_date\", str(last_date))\n\n            mlflow.log_metric(\"expected_price\", expected_price)\n            mlflow.log_metric(\"average_expected_price\", average_expected_price)\n\n            mlflow.statsmodels.log_model(\n                statsmodels_model=model_fit,\n                artifact_path=\"arima_model\",\n                registered_model_name=\"ARIMA_Stock_Predictor\"\n            )\n\n        if mlflow.active_run():\n            mlflow.end_run()\n    except Exception as e:\n        print(f\"MLflow error: {e}\")\n    return {\n        'last_expected_price_arima': expected_price,\n        'average_expected_price_arima': average_expected_price,\n    }", "file_path": "predict_stock/transformers/arima_t.py", "language": "python", "type": "transformer", "uuid": "predict_stock/transformers/arima_t"}, "predict_stock/transformers/lstm.py:transformer:python:predict stock/transformers/lstm": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport mlflow\nimport warnings\nimport tempfile\nimport os\nimport cloudpickle\nfrom datetime import datetime\nimport traceback\n\n@transformer\ndef transform(data):\n    import numpy as np\n    import pandas as pd\n    from sklearn.preprocessing import MinMaxScaler\n    import tensorflow as tf\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import LSTM, Dropout, Dense\n    import mlflow.keras\n    \n    warnings.filterwarnings(\"ignore\") \n    \n    data.index = pd.to_datetime(data.index)\n    close_prices = data['Close'].values.reshape(-1,1)\n    \n    scaler = MinMaxScaler(feature_range=(0,1))\n    scaled_data = scaler.fit_transform(close_prices)\n    \n    last_days = 60\n    train_len = int(len(scaled_data) * 0.8)\n    \n    x_train, y_train = [], []\n    for i in range(last_days, train_len):\n        x_train.append(scaled_data[i-last_days:i, 0])\n        y_train.append(scaled_data[i, 0])\n    x_train, y_train = np.array(x_train), np.array(y_train)\n    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n\n    model = Sequential()\n    model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],1)))\n    model.add(Dropout(0.2))\n    model.add(LSTM(units=50, return_sequences=False))\n    model.add(Dropout(0.2))\n    model.add(Dense(units=25))\n    model.add(Dense(units=1))\n    \n    model.compile(optimizer='adam', loss='mean_squared_error')\n    \n    model.fit(x_train, y_train, epochs=1, batch_size=1, verbose=0)\n    \n    days_to_predict = 30\n    last_60_days = scaled_data[-last_days:]\n    x_input = last_60_days.reshape(1, last_days, 1)\n    \n    predictions = []\n    for _ in range(days_to_predict):\n        pred = model.predict(x_input, verbose=0)\n        predictions.append(pred[0,0])\n        x_input = np.append(x_input[:,1:,:], pred.reshape(1,1,1), axis=1)\n    \n    predictions = scaler.inverse_transform(np.array(predictions).reshape(-1,1))\n    \n    last_date = data.index[-1]\n    future_dates = pd.bdate_range(start=last_date + pd.Timedelta(days=1), periods=days_to_predict)\n    \n    forecast_df = pd.DataFrame(predictions, index=future_dates, columns=['Prediction'])\n    \n    expected_price = forecast_df['Prediction'].iloc[-1]\n    average_expected_price = forecast_df['Prediction'].mean()\n    \n    try:\n        mlflow.set_tracking_uri(\"http://mlflow:5000\")\n        mlflow.set_experiment(f\"Transformer_LSTM_{days_to_predict}d_{last_date.strftime('%Y%m%d')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n        \n        with mlflow.start_run(run_name=f\"Run_{last_date.strftime('%Y-%m-%d')}\"):\n            mlflow.log_param(\"model\", \"LSTM (transformer alternative)\")\n            mlflow.log_param(\"days_to_predict\", days_to_predict)\n            mlflow.log_param(\"last_date\", str(last_date))\n            \n            mlflow.log_metric(\"expected_price\", expected_price)\n            mlflow.log_metric(\"average_expected_price\", average_expected_price)\n            \n            mlflow.keras.log_model(model, artifact_path=\"lstm_model\")\n            \n            run_id = mlflow.active_run().info.run_id\n            model_uri = f\"runs:/{run_id}/lstm_model\"\n            registered_model_name = \"LSTM_Stock_Predictor\"\n            \n            mlflow.register_model(\n                model_uri=model_uri,\n                name=registered_model_name\n            )\n\n    except Exception as e:\n        print(f\"MLflow error: {e}\")\n        traceback.print_exc()\n    \n    return {\n        'last_expected_price_lstm': float(expected_price),\n        'average_expected_price_lstm': float(average_expected_price)\n    }", "file_path": "predict_stock/transformers/lstm.py", "language": "python", "type": "transformer", "uuid": "predict_stock/transformers/lstm"}, "predict_stock/transformers/lstm_t.py:transformer:python:predict stock/transformers/lstm t": {"content": "try:\n    from mage_ai.data_preparation.decorators import transformer\nexcept ImportError:\n    def transformer(func):\n        return func\n\nimport mlflow\nimport warnings\nfrom datetime import datetime\nimport traceback\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dropout, Dense\nimport mlflow.keras\n\n@transformer\ndef transform(data):\n\n    \n    warnings.filterwarnings(\"ignore\") \n    \n    data.index = pd.to_datetime(data.index)\n    close_prices = data['Close'].values.reshape(-1,1)\n    \n    scaler = MinMaxScaler(feature_range=(0,1))\n    scaled_data = scaler.fit_transform(close_prices)\n    \n    last_days = 60\n    train_len = int(len(scaled_data) * 0.8)\n    \n    x_train, y_train = [], []\n    for i in range(last_days, train_len):\n        x_train.append(scaled_data[i-last_days:i, 0])\n        y_train.append(scaled_data[i, 0])\n    x_train, y_train = np.array(x_train), np.array(y_train)\n    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n\n    model = Sequential()\n    model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],1)))\n    model.add(Dropout(0.2))\n    model.add(LSTM(units=50, return_sequences=False))\n    model.add(Dropout(0.2))\n    model.add(Dense(units=25))\n    model.add(Dense(units=1))\n    \n    model.compile(optimizer='adam', loss='mean_squared_error')\n    \n    model.fit(x_train, y_train, epochs=1, batch_size=1, verbose=0)\n    \n    days_to_predict = 30\n    last_60_days = scaled_data[-last_days:]\n    x_input = last_60_days.reshape(1, last_days, 1)\n    \n    predictions = []\n    for _ in range(days_to_predict):\n        pred = model.predict(x_input, verbose=0)\n        predictions.append(pred[0,0])\n        x_input = np.append(x_input[:,1:,:], pred.reshape(1,1,1), axis=1)\n    \n    predictions = scaler.inverse_transform(np.array(predictions).reshape(-1,1))\n    \n    last_date = data.index[-1]\n    future_dates = pd.bdate_range(start=last_date + pd.Timedelta(days=1), periods=days_to_predict)\n    \n    forecast_df = pd.DataFrame(predictions, index=future_dates, columns=['Prediction'])\n    \n    expected_price = forecast_df['Prediction'].iloc[-1]\n    average_expected_price = forecast_df['Prediction'].mean()\n    \n    try:\n        mlflow.set_tracking_uri(\"http://mlflow:5000\")\n        mlflow.set_experiment(f\"Transformer_LSTM_{days_to_predict}d_{last_date.strftime('%Y%m%d')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n        \n        with mlflow.start_run(run_name=f\"Run_{last_date.strftime('%Y-%m-%d')}\"):\n            mlflow.log_param(\"model\", \"LSTM (transformer alternative)\")\n            mlflow.log_param(\"days_to_predict\", days_to_predict)\n            mlflow.log_param(\"last_date\", str(last_date))\n            \n            mlflow.log_metric(\"expected_price\", expected_price)\n            mlflow.log_metric(\"average_expected_price\", average_expected_price)\n            \n            mlflow.keras.log_model(model, artifact_path=\"lstm_model\")\n            \n            run_id = mlflow.active_run().info.run_id\n            model_uri = f\"runs:/{run_id}/lstm_model\"\n            registered_model_name = \"LSTM_Stock_Predictor\"\n            \n            mlflow.register_model(\n                model_uri=model_uri,\n                name=registered_model_name\n            )\n\n    except Exception as e:\n        print(f\"MLflow error: {e}\")\n        traceback.print_exc()\n    \n    return {\n        'last_expected_price_lstm': float(expected_price),\n        'average_expected_price_lstm': float(average_expected_price)\n    }", "file_path": "predict_stock/transformers/lstm_t.py", "language": "python", "type": "transformer", "uuid": "predict_stock/transformers/lstm_t"}, "predict_stock/transformers/montecarlo.py:transformer:python:predict stock/transformers/montecarlo": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport mlflow \nimport cloudpickle\nimport os\nimport tempfile\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport warnings\n@transformer\ndef transform(data, *args, **kwargs):\n\n    warnings.simplefilter(action='ignore', category=FutureWarning)\n\n    data.index = pd.to_datetime(data.index)\n\n    n_days = int(kwargs.get('n_days', 30))\n    n_simulations = int(kwargs.get('n_simulations', 1000))\n\n    prices = data['Close']\n    last_price = float(prices.iloc[-1])\n\n    log_returns = np.log(prices / prices.shift(1)).dropna()\n    mu = log_returns.mean()\n    sigma = log_returns.std()\n    dt = 1 / 252\n    \n    def stock_monte_carlo(start_price, days, mu, sigma):\n        price = np.zeros(days)\n        price[0] = start_price\n        for t in range(1, days):\n            shock = np.random.normal(loc=mu * dt, scale=sigma * np.sqrt(dt))\n            drift = mu * dt\n            price[t] = price[t - 1] + (price[t - 1] * (drift + shock))\n        return price\n\n    simulations = np.zeros((n_days, n_simulations))\n    for i in range(n_simulations):\n        simulations[:, i] = stock_monte_carlo(last_price, n_days, mu, sigma)\n\n    last_date = prices.index[-1]\n    future_dates = pd.bdate_range(start=last_date + pd.Timedelta(days=1), periods=n_days)\n    simulations_df = pd.DataFrame(simulations, index=future_dates)\n\n    last_simulated_price = simulations_df.iloc[-1]\n    average_expected_price = last_simulated_price.mean()\n    conf_interval = np.percentile(last_simulated_price, [5, 95])\n\n    montecarlo_model_data = {\n        'simulations_df': simulations_df,\n        'parameters': {\n            'n_days': n_days,\n            'n_simulations': n_simulations,\n            'mu': mu,\n            'sigma': sigma,\n            'last_price': last_price\n        }\n    }\n    \n    try:\n        mlflow.set_tracking_uri(\"http://mlflow:5000\")\n        mlflow.set_experiment(f\"montecarlo_{n_days}d_{n_simulations}s_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n        \n        with mlflow.start_run(run_name=f\"montecarlo_{n_days}d_{n_simulations}s\"):\n            mlflow.log_param(\"n_days\", n_days)\n            mlflow.log_param(\"n_simulations\", n_simulations)\n            mlflow.log_param(\"expected_price\", average_expected_price)\n            mlflow.log_param(\"mu\", mu)\n            mlflow.log_param(\"sigma\", sigma)\n\n            mlflow.log_metric(\"average_expected_price\", average_expected_price)\n            mlflow.log_metric(\"conf_interval_low\", conf_interval[0])\n            mlflow.log_metric(\"conf_interval_high\", conf_interval[1])\n            \n            with tempfile.NamedTemporaryFile(suffix=\".pkl\", delete=False) as tmp:\n                temp_file_path = tmp.name\n                cloudpickle.dump(montecarlo_model_data, tmp)\n            \n            artifact_path = \"montecarlo_model\"\n            mlflow.log_artifact(temp_file_path, artifact_path=artifact_path)\n            os.remove(temp_file_path)\n        \n            run_id = mlflow.active_run().info.run_id\n            model_uri = f\"runs:/{run_id}/{artifact_path}\"\n            \n            registered_model_name = \"Monte_Carlo_Stock_Predictor\"\n            \n            mlflow.register_model(\n                model_uri=model_uri,\n                name=registered_model_name\n            )\n            print(f\"IC 90%: ${conf_interval[0]:.2f} - ${conf_interval[1]:.2f}\")\n\n    except Exception as e:\n        print(f\"MLflow error: {e}\")\n    \n    return {\n        'last_expected_price_montecarlo': average_expected_price,\n        'average_expected_price_montecarlo': average_expected_price\n    }", "file_path": "predict_stock/transformers/montecarlo.py", "language": "python", "type": "transformer", "uuid": "predict_stock/transformers/montecarlo"}, "predict_stock/transformers/montecarlo_t.py:transformer:python:predict stock/transformers/montecarlo t": {"content": "try:\n    from mage_ai.data_preparation.decorators import transformer\nexcept ImportError:\n    def transformer(func):\n        return func\n\nimport mlflow \nimport cloudpickle\nimport os\nimport tempfile\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport warnings\n@transformer\ndef transform(data, *args, **kwargs):\n\n    warnings.simplefilter(action='ignore', category=FutureWarning)\n\n    data.index = pd.to_datetime(data.index)\n\n    n_days = int(kwargs.get('n_days', 30))\n    n_simulations = int(kwargs.get('n_simulations', 1000))\n\n    prices = data['Close']\n    last_price = float(prices.iloc[-1])\n\n    log_returns = np.log(prices / prices.shift(1)).dropna()\n    mu = log_returns.mean()\n    sigma = log_returns.std()\n    dt = 1 / 252\n    \n    def stock_monte_carlo(start_price, days, mu, sigma):\n        price = np.zeros(days)\n        price[0] = start_price\n        for t in range(1, days):\n            shock = np.random.normal(loc=mu * dt, scale=sigma * np.sqrt(dt))\n            drift = mu * dt\n            price[t] = price[t - 1] + (price[t - 1] * (drift + shock))\n        return price\n\n    simulations = np.zeros((n_days, n_simulations))\n    for i in range(n_simulations):\n        simulations[:, i] = stock_monte_carlo(last_price, n_days, mu, sigma)\n\n    last_date = prices.index[-1]\n    future_dates = pd.bdate_range(start=last_date + pd.Timedelta(days=1), periods=n_days)\n    simulations_df = pd.DataFrame(simulations, index=future_dates)\n\n    last_simulated_price = simulations_df.iloc[-1]\n    average_expected_price = last_simulated_price.mean()\n    conf_interval = np.percentile(last_simulated_price, [5, 95])\n\n    montecarlo_model_data = {\n        'simulations_df': simulations_df,\n        'parameters': {\n            'n_days': n_days,\n            'n_simulations': n_simulations,\n            'mu': mu,\n            'sigma': sigma,\n            'last_price': last_price\n        }\n    }\n    \n    try:\n        mlflow.set_tracking_uri(\"http://mlflow:5000\")\n        mlflow.set_experiment(f\"montecarlo_{n_days}d_{n_simulations}s_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n        \n        with mlflow.start_run(run_name=f\"montecarlo_{n_days}d_{n_simulations}s\"):\n            mlflow.log_param(\"n_days\", n_days)\n            mlflow.log_param(\"n_simulations\", n_simulations)\n            mlflow.log_param(\"expected_price\", average_expected_price)\n            mlflow.log_param(\"mu\", mu)\n            mlflow.log_param(\"sigma\", sigma)\n\n            mlflow.log_metric(\"average_expected_price\", average_expected_price)\n            mlflow.log_metric(\"conf_interval_low\", conf_interval[0])\n            mlflow.log_metric(\"conf_interval_high\", conf_interval[1])\n            \n            with tempfile.NamedTemporaryFile(suffix=\".pkl\", delete=False) as tmp:\n                temp_file_path = tmp.name\n                cloudpickle.dump(montecarlo_model_data, tmp)\n            \n            artifact_path = \"montecarlo_model\"\n            mlflow.log_artifact(temp_file_path, artifact_path=artifact_path)\n            os.remove(temp_file_path)\n        \n            run_id = mlflow.active_run().info.run_id\n            model_uri = f\"runs:/{run_id}/{artifact_path}\"\n            \n            registered_model_name = \"Monte_Carlo_Stock_Predictor\"\n            \n            mlflow.register_model(\n                model_uri=model_uri,\n                name=registered_model_name\n            )\n            print(f\"IC 90%: ${conf_interval[0]:.2f} - ${conf_interval[1]:.2f}\")\n\n    except Exception as e:\n        print(f\"MLflow error: {e}\")\n    \n    return {\n        'last_expected_price_montecarlo': average_expected_price,\n        'average_expected_price_montecarlo': average_expected_price\n    }", "file_path": "predict_stock/transformers/montecarlo_t.py", "language": "python", "type": "transformer", "uuid": "predict_stock/transformers/montecarlo_t"}, "predict_stock/transformers/__init__.py:transformer:python:predict stock/transformers/  init  ": {"content": "", "file_path": "predict_stock/transformers/__init__.py", "language": "python", "type": "transformer", "uuid": "predict_stock/transformers/__init__"}, "predict_stock/pipelines/predict_stock/interactions.yaml:pipeline:yaml:predict stock/pipelines/predict stock/interactions": {"content": "blocks: {}\nlayout: []\n", "file_path": "predict_stock/pipelines/predict_stock/interactions.yaml", "language": "yaml", "type": "pipeline", "uuid": "predict_stock/pipelines/predict_stock/interactions"}, "predict_stock/pipelines/predict_stock/metadata.yaml:pipeline:yaml:predict stock/pipelines/predict stock/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: predict_stock/data_loaders/data_extract.py\n  downstream_blocks:\n  - montecarlo\n  - arima\n  - lstm\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: data_extract\n  retry_config: null\n  status: failed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: data_extract\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_source:\n      path: predict_stock/transformers/montecarlo.py\n  downstream_blocks:\n  - export_data\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: montecarlo\n  retry_config: null\n  status: not_executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - data_extract\n  uuid: montecarlo\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_source:\n      path: predict_stock/transformers/arima.py\n  downstream_blocks:\n  - export_data\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: arima\n  retry_config: null\n  status: not_executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - data_extract\n  uuid: arima\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_source:\n      path: predict_stock/transformers/lstm.py\n  downstream_blocks:\n  - export_data\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: lstm\n  retry_config: null\n  status: not_executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - data_extract\n  uuid: lstm\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_source:\n      path: predict_stock/data_exporters/export_data.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_data\n  retry_config: null\n  status: not_executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - montecarlo\n  - arima\n  - lstm\n  uuid: export_data\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-07-12 20:31:32.507073+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: predict_stock\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers:\n  - name: daily_morning_trigger\n    schedule_interval: 0 9 * * *\n    schedule_type: time\n    start_time: '2025-07-13T09:00:00Z'\n    status: active\nspark_config: {}\ntags: []\ntype: python\nuuid: predict_stock\nvariables_dir: /home/src/mage_data/predict_stock\nwidgets: []\n", "file_path": "predict_stock/pipelines/predict_stock/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "predict_stock/pipelines/predict_stock/metadata"}, "predict_stock/pipelines/predict_stock/__init__.py:pipeline:python:predict stock/pipelines/predict stock/  init  ": {"content": "", "file_path": "predict_stock/pipelines/predict_stock/__init__.py", "language": "python", "type": "pipeline", "uuid": "predict_stock/pipelines/predict_stock/__init__"}}, "custom_block_template": {}, "mage_template": {"data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}, "data_integrations/sources/base:data_loader:sources:Amazon S3:Data integration data loader block for Amazon S3 sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Amazon S3 sources.", "language": "sources", "name": "Amazon S3", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Amazon S3"}}, "data_integrations/sources/base:data_loader:sources:Amplitude:Data integration data loader block for Amplitude sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Amplitude sources.", "language": "sources", "name": "Amplitude", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Amplitude"}}, "data_integrations/sources/base:data_loader:sources:Api:Data integration data loader block for Api sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Api sources.", "language": "sources", "name": "Api", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Api"}}, "data_integrations/sources/base:data_loader:sources:Azure Blob Storage:Data integration data loader block for Azure Blob Storage sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Azure Blob Storage sources.", "language": "sources", "name": "Azure Blob Storage", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Azure Blob Storage"}}, "data_integrations/sources/base:data_loader:sources:BigQuery:Data integration data loader block for BigQuery sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for BigQuery sources.", "language": "sources", "name": "BigQuery", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "BigQuery"}}, "data_integrations/sources/base:data_loader:sources:Chargebee:Data integration data loader block for Chargebee sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Chargebee sources.", "language": "sources", "name": "Chargebee", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Chargebee"}}, "data_integrations/sources/base:data_loader:sources:Commercetools:Data integration data loader block for Commercetools sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Commercetools sources.", "language": "sources", "name": "Commercetools", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Commercetools"}}, "data_integrations/sources/base:data_loader:sources:Couchbase:Data integration data loader block for Couchbase sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Couchbase sources.", "language": "sources", "name": "Couchbase", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Couchbase"}}, "data_integrations/sources/base:data_loader:sources:Datadog:Data integration data loader block for Datadog sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Datadog sources.", "language": "sources", "name": "Datadog", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Datadog"}}, "data_integrations/sources/base:data_loader:sources:Dremio:Data integration data loader block for Dremio sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Dremio sources.", "language": "sources", "name": "Dremio", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Dremio"}}, "data_integrations/sources/base:data_loader:sources:DynamoDB:Data integration data loader block for DynamoDB sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for DynamoDB sources.", "language": "sources", "name": "DynamoDB", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "DynamoDB"}}, "data_integrations/sources/base:data_loader:sources:Facebook Ads:Data integration data loader block for Facebook Ads sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Facebook Ads sources.", "language": "sources", "name": "Facebook Ads", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Facebook Ads"}}, "data_integrations/sources/base:data_loader:sources:Freshdesk:Data integration data loader block for Freshdesk sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Freshdesk sources.", "language": "sources", "name": "Freshdesk", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Freshdesk"}}, "data_integrations/sources/base:data_loader:sources:Front:Data integration data loader block for Front sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Front sources.", "language": "sources", "name": "Front", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Front"}}, "data_integrations/sources/base:data_loader:sources:GitHub:Data integration data loader block for GitHub sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for GitHub sources.", "language": "sources", "name": "GitHub", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "GitHub"}}, "data_integrations/sources/base:data_loader:sources:Google Ads:Data integration data loader block for Google Ads sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Google Ads sources.", "language": "sources", "name": "Google Ads", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Google Ads"}}, "data_integrations/sources/base:data_loader:sources:Google Analytics:Data integration data loader block for Google Analytics sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Google Analytics sources.", "language": "sources", "name": "Google Analytics", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Google Analytics"}}, "data_integrations/sources/base:data_loader:sources:Google Search Console:Data integration data loader block for Google Search Console sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Google Search Console sources.", "language": "sources", "name": "Google Search Console", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Google Search Console"}}, "data_integrations/sources/base:data_loader:sources:Google Sheets:Data integration data loader block for Google Sheets sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Google Sheets sources.", "language": "sources", "name": "Google Sheets", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Google Sheets"}}, "data_integrations/sources/base:data_loader:sources:HubSpot:Data integration data loader block for HubSpot sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for HubSpot sources.", "language": "sources", "name": "HubSpot", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "HubSpot"}}, "data_integrations/sources/base:data_loader:sources:Intercom:Data integration data loader block for Intercom sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Intercom sources.", "language": "sources", "name": "Intercom", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Intercom"}}, "data_integrations/sources/base:data_loader:sources:Knowi:Data integration data loader block for Knowi sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Knowi sources.", "language": "sources", "name": "Knowi", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Knowi"}}, "data_integrations/sources/base:data_loader:sources:LinkedIn Ads:Data integration data loader block for LinkedIn Ads sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for LinkedIn Ads sources.", "language": "sources", "name": "LinkedIn Ads", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "LinkedIn Ads"}}, "data_integrations/sources/base:data_loader:sources:Microsoft SQL Server:Data integration data loader block for Microsoft SQL Server sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Microsoft SQL Server sources.", "language": "sources", "name": "Microsoft SQL Server", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Microsoft SQL Server", "uuid": "mssql"}}, "data_integrations/sources/base:data_loader:sources:Mode:Data integration data loader block for Mode sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Mode sources.", "language": "sources", "name": "Mode", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Mode"}}, "data_integrations/sources/base:data_loader:sources:Monday:Data integration data loader block for Monday sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Monday sources.", "language": "sources", "name": "Monday", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Monday"}}, "data_integrations/sources/base:data_loader:sources:MongoDB:Data integration data loader block for MongoDB sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for MongoDB sources.", "language": "sources", "name": "MongoDB", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "MongoDB"}}, "data_integrations/sources/base:data_loader:sources:MySQL:Data integration data loader block for MySQL sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for MySQL sources.", "language": "sources", "name": "MySQL", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "MySQL"}}, "data_integrations/sources/base:data_loader:sources:OracleDB:Data integration data loader block for OracleDB sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for OracleDB sources.", "language": "sources", "name": "OracleDB", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "OracleDB"}}, "data_integrations/sources/base:data_loader:sources:Outreach:Data integration data loader block for Outreach sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Outreach sources.", "language": "sources", "name": "Outreach", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Outreach"}}, "data_integrations/sources/base:data_loader:sources:Paystack:Data integration data loader block for Paystack sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Paystack sources.", "language": "sources", "name": "Paystack", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Paystack"}}, "data_integrations/sources/base:data_loader:sources:Pipedrive:Data integration data loader block for Pipedrive sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Pipedrive sources.", "language": "sources", "name": "Pipedrive", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Pipedrive"}}, "data_integrations/sources/base:data_loader:sources:PostgreSQL:Data integration data loader block for PostgreSQL sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for PostgreSQL sources.", "language": "sources", "name": "PostgreSQL", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "PostgreSQL"}}, "data_integrations/sources/base:data_loader:sources:Postmark:Data integration data loader block for Postmark sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Postmark sources.", "language": "sources", "name": "Postmark", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Postmark"}}, "data_integrations/sources/base:data_loader:sources:PowerBI:Data integration data loader block for PowerBI sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for PowerBI sources.", "language": "sources", "name": "PowerBI", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "PowerBI"}}, "data_integrations/sources/base:data_loader:sources:Redshift:Data integration data loader block for Redshift sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Redshift sources.", "language": "sources", "name": "Redshift", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Redshift"}}, "data_integrations/sources/base:data_loader:sources:Salesforce:Data integration data loader block for Salesforce sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Salesforce sources.", "language": "sources", "name": "Salesforce", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Salesforce"}}, "data_integrations/sources/base:data_loader:sources:Sftp:Data integration data loader block for Sftp sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Sftp sources.", "language": "sources", "name": "Sftp", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Sftp"}}, "data_integrations/sources/base:data_loader:sources:Snowflake:Data integration data loader block for Snowflake sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Snowflake sources.", "language": "sources", "name": "Snowflake", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Snowflake"}}, "data_integrations/sources/base:data_loader:sources:Stripe:Data integration data loader block for Stripe sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Stripe sources.", "language": "sources", "name": "Stripe", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Stripe"}}, "data_integrations/sources/base:data_loader:sources:Tableau:Data integration data loader block for Tableau sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Tableau sources.", "language": "sources", "name": "Tableau", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Tableau"}}, "data_integrations/sources/base:data_loader:sources:Twitter Ads:Data integration data loader block for Twitter Ads sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Twitter Ads sources.", "language": "sources", "name": "Twitter Ads", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Twitter Ads"}}, "data_integrations/sources/base:data_loader:sources:Zendesk:Data integration data loader block for Zendesk sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Zendesk sources.", "language": "sources", "name": "Zendesk", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Zendesk"}}, "data_integrations/destinations/base:data_exporter:destinations:Amazon S3:Data integration data exporter block for Amazon S3 destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Amazon S3 destinations.", "language": "destinations", "name": "Amazon S3", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Amazon S3"}}, "data_integrations/destinations/base:data_exporter:destinations:BigQuery:Data integration data exporter block for BigQuery destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for BigQuery destinations.", "language": "destinations", "name": "BigQuery", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "BigQuery"}}, "data_integrations/destinations/base:data_exporter:destinations:Clickhouse:Data integration data exporter block for Clickhouse destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Clickhouse destinations.", "language": "destinations", "name": "Clickhouse", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Clickhouse"}}, "data_integrations/destinations/base:data_exporter:destinations:Delta Lake Azure:Data integration data exporter block for Delta Lake Azure destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Delta Lake Azure destinations.", "language": "destinations", "name": "Delta Lake Azure", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Delta Lake Azure"}}, "data_integrations/destinations/base:data_exporter:destinations:Delta Lake S3:Data integration data exporter block for Delta Lake S3 destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Delta Lake S3 destinations.", "language": "destinations", "name": "Delta Lake S3", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Delta Lake S3"}}, "data_integrations/destinations/base:data_exporter:destinations:Elasticsearch:Data integration data exporter block for Elasticsearch destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Elasticsearch destinations.", "language": "destinations", "name": "Elasticsearch", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Elasticsearch"}}, "data_integrations/destinations/base:data_exporter:destinations:Google Cloud Storage:Data integration data exporter block for Google Cloud Storage destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Google Cloud Storage destinations.", "language": "destinations", "name": "Google Cloud Storage", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Google Cloud Storage"}}, "data_integrations/destinations/base:data_exporter:destinations:Kafka:Data integration data exporter block for Kafka destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Kafka destinations.", "language": "destinations", "name": "Kafka", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Kafka"}}, "data_integrations/destinations/base:data_exporter:destinations:MongoDB:Data integration data exporter block for MongoDB destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for MongoDB destinations.", "language": "destinations", "name": "MongoDB", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "MongoDB"}}, "data_integrations/destinations/base:data_exporter:destinations:Microsoft SQL Server:Data integration data exporter block for Microsoft SQL Server destinations (MSSQL).": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Microsoft SQL Server destinations (MSSQL).", "language": "destinations", "name": "Microsoft SQL Server", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"module_name": "MSSQL", "name": "Microsoft SQL Server", "uuid": "mssql"}}, "data_integrations/destinations/base:data_exporter:destinations:MySQL:Data integration data exporter block for MySQL destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for MySQL destinations.", "language": "destinations", "name": "MySQL", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "MySQL"}}, "data_integrations/destinations/base:data_exporter:destinations:Opensearch:Data integration data exporter block for Opensearch destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Opensearch destinations.", "language": "destinations", "name": "Opensearch", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Opensearch"}}, "data_integrations/destinations/base:data_exporter:destinations:OracleDB:Data integration data exporter block for OracleDB destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for OracleDB destinations.", "language": "destinations", "name": "OracleDB", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "OracleDB"}}, "data_integrations/destinations/base:data_exporter:destinations:PostgreSQL:Data integration data exporter block for PostgreSQL destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for PostgreSQL destinations.", "language": "destinations", "name": "PostgreSQL", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "PostgreSQL"}}, "data_integrations/destinations/base:data_exporter:destinations:Redshift:Data integration data exporter block for Redshift destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Redshift destinations.", "language": "destinations", "name": "Redshift", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Redshift"}}, "data_integrations/destinations/base:data_exporter:destinations:Salesforce:Data integration data exporter block for Salesforce destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Salesforce destinations.", "language": "destinations", "name": "Salesforce", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Salesforce"}}, "data_integrations/destinations/base:data_exporter:destinations:Snowflake:Data integration data exporter block for Snowflake destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Snowflake destinations.", "language": "destinations", "name": "Snowflake", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Snowflake"}}, "data_integrations/destinations/base:data_exporter:destinations:Trino:Data integration data exporter block for Trino destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Trino destinations.", "language": "destinations", "name": "Trino", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Trino"}}}}
